# Speaker diarization

## Prerequisites
1. PyTorch
2. Keras
3. TensorFlow
4. PyAudio

## Outline
Thanks to the authors of VGG, they are kind enough to provide the code and pre-trained model.
Their paper can refer to [UTTERANCE-LEVEL AGGREGATION FOR SPEAKER RECOGNITION IN THE WILD](https://arxiv.org/pdf/1902.10107.pdf)</br>
It's a novel idea that combines `netvlad/ghostvlad` which popularly used in image recognition to speaker recognition, the state-of-the-art in the past was `i-vector` based, which depended on the `GMM` model and `pLDA`.

About VGG speaker model, I have re-implemented in tensorflow, [ghostvlad-speaker](https://github.com/taylorlu/ghostvlad-speaker) and corresponding pretrained model.

This project only shows how to generate speaker embeddings using pre-trained model for uis-rnn training in later.</br>
The training project link to [VGG-Speaker-Recognition](https://github.com/WeidiXie/VGG-Speaker-Recognition)
#### Dataset
  1. Telephone calls from call center (this is private data and i can't insert link for download this dataset).
  2. Interview, performances, panel discussions from YouTube.
  3. Movies with a subtitle file. In this case, such a dataset is marked up unlike the previous options,
     you just need to cut it into pieces.
  
  **How to generate speaker embeddings for the next training stage:**</br>
  `python generate_embeddings.py` </br>
  You may need to change the dataset path by your own.
 
### 2. Speaker diarization.</br>
![diarization](https://github.com/taylorlu/Speaker-Diarization/blob/master/resources/diarization.gif)
#### Training
    python train.py
The speaker embeddings generated by vgg are all non-negative vectors, and contained many **zero** elements. The uis-rnn seems abnormally deal with these data somehow, shows as below

    Iter: 0  	Training Loss: nan    
    Negative Log Likelihood: 7.3020	Sigma2 Prior: nan	Regularization: 0.0007
    Iter: 10  	Training Loss: nan    
    Negative Log Likelihood: nan	Sigma2 Prior: nan	Regularization: nan
    Iter: 20  	Training Loss: nan    
    Negative Log Likelihood: nan	Sigma2 Prior: nan	Regularization: nan
        
When I added an insignificant bias (e.g. 0.00001) to each element of vectors, error disappeared.

    Iter: 0  	Training Loss: -581.8732    
    Negative Log Likelihood: 7.0125	Sigma2 Prior: -588.8864	Regularization: 0.0007
    Iter: 10  	Training Loss: -614.1193    
    Negative Log Likelihood: 1.7536	Sigma2 Prior: -615.8737	Regularization: 0.0007
    Iter: 20  	Training Loss: -644.9244    
    Negative Log Likelihood: 1.7123	Sigma2 Prior: -646.6375	Regularization: 0.0007

#### Segmentation
`python speaker_diarization.py`

The result is showing as below (for 3 speakers):

    ========= 0 =========
    0:00.288 ==> 0:04.406
    0:07.699 ==> 0:16.461
    0:33.921 ==> 0:35.8
    ========= 1 =========
    0:04.406 ==> 0:07.699
    0:16.461 ==> 0:19.594
    0:30.371 ==> 0:33.921
    0:41.19 ==> 0:44.185
    ========= 2 =========
    0:19.594 ==> 0:30.371
    0:35.8 ==> 0:41.19

The final result is influenced by the size of each window and the overlap rate.
When the overlap is too large, the uis-rnn perhaps generates fewer speakers since the speaker embeddings changed smoothly, otherwise will generate more speakers.
And also, the window size cannot be too short, it must contain enough information to generate more discrimitive speaker embeddings.


## Embeddings
I used TensorBoard for visualizing embeddings. For example, I generated one wav from 10 different speakers.

<div align="center">
    <img src="assets/tensorboard.png">
</div>

Go to `ghostvlad/generate_embeddings.py` and call "visualize()" method before print epoch number. 
After this run `tensorboard --logdir="projections" --port=8080`